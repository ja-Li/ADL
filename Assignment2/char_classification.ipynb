{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR ab\n",
    "# HIDDEN_SIZE = 16\n",
    "# BATCH_SIZE = 100\n",
    "# N_LAYER = 1\n",
    "# N_EPOCHS = 60\n",
    "# N_CHARS = 128\n",
    "# USE_GPU = False\n",
    "# learning_rate = 0.005\n",
    "# # For abc\n",
    "# HIDDEN_SIZE = 32\n",
    "# BATCH_SIZE = 200\n",
    "# N_LAYER = 1\n",
    "# N_EPOCHS = 22\n",
    "# N_CHARS = 128\n",
    "# USE_GPU = False\n",
    "# learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR ab\n",
    "# HIDDEN_SIZE = 64\n",
    "# BATCH_SIZE = 100\n",
    "# N_LAYER = 1\n",
    "# N_EPOCHS = 7\n",
    "# N_CHARS = 128\n",
    "# USE_GPU = False\n",
    "# learning_rate = 0.01\n",
    "# For abc\n",
    "HIDDEN_SIZE = 64\n",
    "BATCH_SIZE = 100\n",
    "N_LAYER = 1\n",
    "N_EPOCHS = 10\n",
    "N_CHARS = 128\n",
    "USE_GPU = False\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharsDataset(Dataset):\n",
    "    def __init__(self, is_train_set=True):\n",
    "        self.all_letters = \"abc\"\n",
    "        self.filenames = f'./data/{self.all_letters}train/*.txt' if is_train_set else f'./data/{self.all_letters}test/*.txt'\n",
    "        self.category_lines = {\"0\":[],\"1\":[]}\n",
    "        self.all_categories = []\n",
    "        self.lines = []\n",
    "        for filename in glob.glob(self.filenames):\n",
    "            category = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.all_categories.append(category)\n",
    "            lines = self.readLines(filename)\n",
    "            # for val_length in val_lengths:\n",
    "            #     val_lines = list(filter(lambda s: len(s) == val_length, lines))\n",
    "            #     # self.category_validation_lines[category] += val_lines\n",
    "            # train_lines = [tl for tl in lines if tl not in self.category_validation_lines[category]]\n",
    "            self.category_lines[category] = lines\n",
    "            self.lines += lines\n",
    "        self.len = len(self.lines)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.category_lines[str(index%len(self.all_categories))][index // len(self.all_categories)]\n",
    "        label_index = int(list(self.category_lines.keys())[index%len(self.all_categories)])\n",
    "        return item, label_index \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def unicodeToAscii(self, s):\n",
    "        lines = ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in self.all_letters\n",
    "        )\n",
    "        return lines\n",
    "\n",
    "    def readLines(self, filename):\n",
    "        lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [self.unicodeToAscii(line) for line in lines]\n",
    "\n",
    "    def get_categorires_num(self):\n",
    "        return len(self.all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CharsDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "N_LABELS = trainset.get_categorires_num()\n",
    "\n",
    "testset = CharsDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=False):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = torch.nn.RNN(hidden_size, hidden_size, n_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size*self.n_directions, output_size)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*self.n_directions, batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()  # input.shape:BxS->SxB\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        # pack them up\n",
    "        rnn_input = pack_padded_sequence(embedding, seq_lengths)\n",
    "\n",
    "        output , hidden = self.rnn(rnn_input, hidden)\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        return self.fc(hidden_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size*self.n_directions, output_size)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*self.n_directions, batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()  # input.shape:BxS->SxB\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        c0 = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        # pack them up\n",
    "        lstm_input = pack_padded_sequence(embedding, seq_lengths)\n",
    "\n",
    "        lstm_output , (hidden, ct) = self.lstm(lstm_input, (hidden, c0))\n",
    "        return self.fc(hidden[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_LABELS, N_LAYER)\n",
    "\n",
    "classifier = LSTMClassifier(N_CHARS, HIDDEN_SIZE, N_LABELS, N_LAYER)\n",
    "\n",
    "if USE_GPU:\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    classifier.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert name to tensor\n",
    "def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "def line2list(line):\n",
    "    arr = [ord(c) for c in line]\n",
    "    return arr, len(arr)\n",
    "\n",
    "def make_tensors(lines, labels):\n",
    "    sequences_and_lengths = [line2list(line) for line in lines]\n",
    "    chars_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "    \n",
    "    labels = labels.long()\n",
    "\n",
    "    # make tensor of name, Batchsize x Seqlen, padding\n",
    "    seq_tensor = torch.zeros(len(chars_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(chars_sequences, seq_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    labels = labels[perm_idx]\n",
    "\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"grad_clipping\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    classifier.train()\n",
    "    total_loss = 0.0\n",
    "    for i, (lines, labels) in enumerate(trainloader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(lines, labels)\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_clipping(classifier, 1)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    loss_save.append(total_loss / len(trainset))\n",
    "    print(f'[{time_since(start)}] Epoch {epoch} , loss={total_loss / len(trainset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictModel():\n",
    "    acc_dict = {i: torch.zeros(N_LABELS, N_LABELS) for i in range(21, 51)}\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    print('evaluating tarined model ...')\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (lines, labels) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(lines, labels)\n",
    "            test_length = np.count_nonzero(inputs, axis=1)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for i, key in enumerate(test_length):\n",
    "                acc_dict[key][target[i]][pred[i]] += 1\n",
    "        for i in range(21, 51):\n",
    "            acc = (acc_dict[i][0][0] + acc_dict[i][1][1]) / acc_dict[i].sum()\n",
    "            print(f\"length: {i} and accuracy is {acc}\")\n",
    "        \n",
    "        percent = '%.2f' % (100*correct/total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "\n",
    "    return acc_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainin for 10 epochs....\n",
      "[0m 1s] Epoch 1 , loss=0.0037567878844216467\n",
      "[0m 2s] Epoch 2 , loss=0.0001329926607257221\n",
      "[0m 3s] Epoch 3 , loss=1.199590184551198e-05\n",
      "[0m 4s] Epoch 4 , loss=5.93155733513413e-06\n",
      "[0m 5s] Epoch 5 , loss=2.5064163921342696e-06\n",
      "[0m 6s] Epoch 6 , loss=1.3681934338819702e-06\n",
      "[0m 8s] Epoch 7 , loss=9.011031983391149e-07\n",
      "[0m 9s] Epoch 8 , loss=6.497006193967536e-07\n",
      "[0m 10s] Epoch 9 , loss=5.051642843682202e-07\n",
      "[0m 11s] Epoch 10 , loss=4.0460628133587305e-07\n",
      "evaluating tarined model ...\n",
      "length: 21 and accuracy is 1.0\n",
      "length: 22 and accuracy is 1.0\n",
      "length: 23 and accuracy is 1.0\n",
      "length: 24 and accuracy is 1.0\n",
      "length: 25 and accuracy is 1.0\n",
      "length: 26 and accuracy is 1.0\n",
      "length: 27 and accuracy is 1.0\n",
      "length: 28 and accuracy is 1.0\n",
      "length: 29 and accuracy is 1.0\n",
      "length: 30 and accuracy is 1.0\n",
      "length: 31 and accuracy is 1.0\n",
      "length: 32 and accuracy is 1.0\n",
      "length: 33 and accuracy is 1.0\n",
      "length: 34 and accuracy is 1.0\n",
      "length: 35 and accuracy is 1.0\n",
      "length: 36 and accuracy is 1.0\n",
      "length: 37 and accuracy is 1.0\n",
      "length: 38 and accuracy is 1.0\n",
      "length: 39 and accuracy is 1.0\n",
      "length: 40 and accuracy is 1.0\n",
      "length: 41 and accuracy is 1.0\n",
      "length: 42 and accuracy is 1.0\n",
      "length: 43 and accuracy is 1.0\n",
      "length: 44 and accuracy is 1.0\n",
      "length: 45 and accuracy is 1.0\n",
      "length: 46 and accuracy is 1.0\n",
      "length: 47 and accuracy is 1.0\n",
      "length: 48 and accuracy is 1.0\n",
      "length: 49 and accuracy is 1.0\n",
      "length: 50 and accuracy is 1.0\n",
      "Test set: Accuracy 6000/6000 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3384afb100>]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('Trainin for %d epochs....' % N_EPOCHS)\n",
    "\n",
    "loss_save = []\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    trainModel()\n",
    "\n",
    "acc_dict=predictModel()\n",
    "\n",
    "acc = [(acc_dict[i][0][0] + acc_dict[i][1][1]) / acc_dict[i].sum() for i in range(21, 51)]\n",
    "plt.xlim(20,50)\n",
    "plt.plot(np.arange(21,51), acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "22 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "23 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "24 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "25 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "26 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "27 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "28 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "29 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "30 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "31 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "32 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "33 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "34 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "35 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "36 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "37 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "38 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "39 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "40 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "41 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "42 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "43 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "44 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "45 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "46 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "47 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "48 tensor([[100.,   0.],\n",
      "        [  0., 300.]])\n",
      "49 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n",
      "50 tensor([[100.,   0.],\n",
      "        [  0.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(21,51):\n",
    "    print(i, acc_dict[i])\n",
    "# acc = [(_acc_check[i][0][0] + _acc_check[i][1][1]) / _acc_check[i].sum() for i in range(21, 51)]\n",
    "# plt.plot(np.arange(21,51), acc)\n",
    "# # plt.ylim(0.8, 1.05)\n",
    "# plt.xlim(20,50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3551ac1481d9ecdebb081487cb77eb71c94cc108f5d5a612da6892cfe5edac48"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
